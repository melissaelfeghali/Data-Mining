---
title: "Final Project - Data Mining"
author: "Melissa El Feghali"
date: "12/15/2020"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true 
    theme: darkly
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# First Analysis 

```{r}
pd_data <- read.csv("~/GitLab/BIF524_Fall20_ElFeghali_Melissa/Final_Project/Parkinson_data.csv", header = TRUE, sep = ",", dec = ".")
```

Check for missing values 
```{r}
sum(is.na(pd_data))
```

## Explore data 

```{r}
dim(pd_data)
```
We have 195 rows and 24 columns (attributes). 

```{r}
str(pd_data)
```

We know that status is a categorical variable.
```{r}
pd_data$status <- as.factor(pd_data$status)
```

The attribute name does not contribute in building our model, thus why we will remove it.
```{r}
pd_data_1 <- pd_data[, -1]
```

```{r}
summary(pd_data_1)
```
```{r}
standard_deviations <- sapply(pd_data_1[, -17], sd)
summary_sd <- matrix(standard_deviations, nrow = 1, ncol = 22, byrow = TRUE)
rownames(summary_sd) <- "SD"
colnames(summary_sd) <- colnames(pd_data_1[,-17])
summary_sd
```
A low standard deviation indicates that the data points tend to be close to the mean of the data set, while a high standard deviation indicates that the data points are spread out over a wider range of values.
From the summary table of standard deviations, we can conclude that all the columns in the data have a spread closer to the mean, except the three columns MDVP:Fo(Hz), MDVP:Fhi(Hz) and MDVP:Flo(Hz). 

```{r}
par(mfrow=c(2,3))
boxplot(pd_data_1[,1:4])
boxplot(pd_data_1[,5:8])
boxplot(pd_data_1[,9:12])
boxplot(pd_data_1[,13:16])
boxplot(pd_data_1[,17:20])
boxplot(pd_data_1[,21:23])
```
We notice from the boxplots that there are outliers in almost all the columns except the MDVP:Fo(Hz), RPDE and DFA columns. Most of the columns with outliers are positively skewed except HNR which is negatively skewed. In the column 'spread2', we can see the outliers present on both sides.

```{r}
plot(pd_data_1$status, xlab = "Status", ylab = "Count", main = "Distribution of status attribute", col = "darkblue")
```
There are 147 data points that indicate the presence of Parkinson's disease and 48 data points that indicate the absence of the disease. This confirms that the data set is skewed.

We can see the relationship of individual data points and relationships of clusters. This is achieved by successively joining small clusters to each other based on the inter-cluster distance.

```{r}
d <- dist(pd_data_1)
hc <- hclust(d, method="average")
plot(hc)
```

Our response variable is qualitative, thus we need to fit a classification model. There are many possible classification techniques, or classifiers, that we can use to predict a qualitative response. We will use three: logistic regression, linear discriminant analysis, and K-nearest neighbors.

In order to better assess the accuracy of the models, we will fit the models using part of the data, and then examine how well they predict the held out data.

```{r}
library(caTools)

# ID : 201803573 last 4-digit are 3573
set.seed(3573)

sample <- sample.split(pd_data_1, SplitRatio = 2/3)
pd_train = subset(pd_data_1, sample == TRUE)
pd_test  = subset(pd_data_1, sample == FALSE) 
```

## Linear Models {.tabset .tabset-fade .tabset-pills}


```{r}
cross_validation <- function(full_data, model_type, kfolds,
                             logistic_formula = NULL) {
  
  ## Define fold_ids in exactly the same way as before
  fold_ids      <- rep(seq(kfolds), 
                       ceiling(nrow(full_data) / kfolds))
  fold_ids      <- fold_ids[1:nrow(full_data)]
  fold_ids      <- sample(fold_ids, length(fold_ids))
  table(fold_ids)
  
  ## Initialize a vector to store CV error
  CV_error_vec  <- vector(length = kfolds, mode = "numeric")
  
  ## Loop through the folds
  for (k in 1:kfolds){
    if (model_type == "logistic") {
      logistic_model    <- glm(logistic_formula,
                               data = full_data[which(fold_ids != k),],
                               family = binomial)
      logistic_pred     <- predict(logistic_model,
                                   full_data[which(fold_ids == k),],
                                   type = "response")
      class_pred        <- as.numeric(logistic_pred > 0.5)
      
    } else if (model_type == "LDA") {
      lda_model         <- lda(full_data[which(fold_ids != k),-9], 
                               full_data[which(fold_ids != k),9])
      lda_pred          <- predict(lda_model, 
                                   full_data[which(fold_ids == k),-9])
      class_pred        <- lda_pred$class
      
    } else if (model_type == "QDA") {
      qda_model         <- qda(full_data[which(fold_ids != k),-9], 
                               full_data[which(fold_ids != k),9])
      qda_pred          <- predict(qda_model, 
                                   full_data[which(fold_ids == k),-9])
      class_pred        <- qda_pred$class
      
    }
    
    CV_error_vec[k]     <- mean(class_pred != full_data[which(fold_ids == k),9])
  }
  return(CV_error_vec)
}
```

### Logistic Regression 

We now fit a logistic regression model using only the subset of the observations. 
```{r}
attach(pd_data_1)
glm.fits <- glm(status~. , data = pd_train, family = binomial )
summary(glm.fits)
```
The smallest p-value here is associated with spread2. 

```{r}
glm.probs <- predict(glm.fits, pd_test, type = "response")
glm.pred <- rep(0, dim(pd_test)[1])
glm.pred[glm.probs>.5] <- 1
table(glm.pred, pd_test$status)
```
```{r}
logistic_error <- mean(glm.pred != pd_test$status)
logistic_error
```
The test set error rate of the logistic model is 22% 

```{r}
library(boot)
cv.err = cv.glm(pd_train, glm.fits)
logistic_CV_error <- cv.err$delta
logistic_CV_error
```

### Linear Discriminant Analysis 

Now, we will perform LDA on the training data set.

```{r}
library(MASS)
lda.fit <- lda(status~., data = pd_train[, -5])
lda.fit
```

```{r}
lda.pred <- predict(lda.fit, pd_test)
lda.class <- lda.pred$class
table(lda.class, pd_test$status)
```
```{r}
lda_error <- mean(lda.class != pd_test$status)
lda_error
```
The test set error rate of the LDA model is 19% 

### Quadratic Discriminant Analysis 

We will now fit a QDA model to the training data set. 

```{r}
qda.fit <- qda(status~. , data = pd_train)
qda.fit
```
The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors.

```{r}
qda.class <- predict(qda.fit, pd_test)$class
table(qda.class, pd_test$status)
```

```{r}
qda_error <- mean(qda.class != pd_test$status)
qda_error
```
The test set error rate of the QDA model is 19%, equal to that of the initial LDA model.

### K-Nearest Neighbors 

We will now perform KNN. 
```{r}
library(class)

set.seed(3573)
knn.pred <- knn(pd_train, pd_test, pd_train$status, k = 1)
table(knn.pred, pd_test$status)
```
```{r}
knn_1_error <- 1-(12+46)/68
knn_1_error
```
The test set error rate using K=1 is 14.7 %

We now repeat the analysis using K = 3
```{r}
set.seed(3573)
knn.pred <- knn(pd_train, pd_test, pd_train$status, k = 3)
table(knn.pred, pd_test$status)
```
```{r}
knn_3_error <- 1-(5+45)/68
knn_3_error
```
Increasing K does not provides us with better results. 


## Linear Model Selection and Regularization 

Perhaps by removing the variables that appear not to be helpful in predicting status, we can obtain a more effective model. Using predictors that have no relationship with the response tends to cause a deterioration in the test error rate, since such predictors cause an increase in variance without a corresponding decrease in bias. Thus, removing such predictors may in turn yield an improvement.

We will use certain fitting procedures that can yield better prediction accuracy and model interpretability than the plain least squares. 

### Subset Selection {.tabset .tabset-fade .tabset-pills}

This approach involves identifying a subset of the p predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.

#### Best Subset Selection 

We apply regsubsets() to the training set in order to perform best subset selection.
```{r}
library(leaps)
regfit.best <- regsubsets(status~., data = pd_train, nvmax = 22)
```

We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data. Then, we run a loop, and for each size i, we extract the coefficients from regfit.best for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.

```{r}
test.mat <- model.matrix(status~., data = pd_test)
```
```{r}
val.errors <- rep(NA, 22)
for(i in 1:22){
  coefi <- coef(regfit.best, id = i)
  pred <- test.mat[, names(coefi)]%*%coefi
  val.errors[i] <- mean(as.numeric(pd_test$status)-pred^2)
}
which.min(val.errors)
```
We find that the best model is the one that contains 6 variables. 

Finally, we perform best subset selection on the full data set, and select the best six-variable model. 
```{r}
regfit.best <- regsubsets(status~., data = pd_data_1, nvmax = 22)
coef(regfit.best, 6)
```

There is no predict() method for regsubsets(), thus we can capture our steps above and write our own predict method to use again later on.

```{r}
predict.regsubsets <- function (object ,newdata ,id ,...){
  form <- as.formula (object$call [[2]])
  mat <- model.matrix (form ,newdata )
  coefi <- coef(object ,id=id)
  xvars <- names (coefi )
  mat[,xvars]%*% coefi
}
```

We now try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform best subset selection within each of the k training sets.

```{r}
k <- 10
set.seed(3573)
folds <- sample(1:k, nrow(pd_data_1), replace = TRUE)
cv.errors <- matrix(NA, k, 22, dimnames = list(NULL, paste(1:22)))
```

Now we write a for loop that performs cross-validation. In the jth fold, the elements of folds that equal j are in the test set, and the remainder are in the training set.
```{r}
for(j in 1:k){
  best.fit <- regsubsets(status~., data = pd_data_1[folds!=j,], nvmax = 22)
  for(i in 1:22){
    pred <- predict.regsubsets(best.fit, pd_data_1[folds==j,], id=i)
    cv.errors[j,i] <- mean((as.numeric(pd_data_1$status)[folds==j]-pred)^2)
  }
}
```

We use the apply() function to average over the columns of this matrix in order to obtain a vector for which the jth element is the cross-validation error for the j-variable model. 

```{r}
mean.cv.errors <- apply(cv.errors, 2, mean)
plot(mean.cv.errors, type = "b")
```

Using a 13-variable model would be a good choice according to the cross-validation. We now perform best subset selection on the full data set in order to obtain the 13-variable model. 
```{r}
reg.best <- regsubsets(status~., data = pd_data_1, nvmax = 22)
coef(reg.best, 13)
```

#### Forward Stepwise Selection 

```{r}
regfit.fwd.best <- regsubsets(status~., data = pd_train, nvmax = 22, method = "forward")
```

```{r}
val.errors <- rep(NA, 22)
for(i in 1:22){
  coefi <- coef(regfit.fwd.best, id = i)
  pred <- test.mat[, names(coefi)]%*%coefi
  val.errors[i] <- mean(as.numeric(pd_test$status)-pred^2)
}
which.min(val.errors)
```

```{r}
regfit.fwd.best <- regsubsets(status~., data = pd_data_1, nvmax = 22, method = "forward")
coef(regfit.fwd.best, 1)
```

We now try to choose among the models of different sizes using crossvalidation.
```{r}
for(j in 1:k){
  best.fit <- regsubsets(status~., data = pd_data_1[folds!=j,], nvmax = 22, method = "forward")
  for(i in 1:22){
    pred <- predict.regsubsets(best.fit, pd_data_1[folds==j,], id=i)
    cv.errors[j,i] <- mean((as.numeric(pd_data_1$status)[folds==j]-pred)^2)
  }
}
```
```{r}
mean.cv.errors <- apply(cv.errors, 2, mean)
plot(mean.cv.errors, type = "b")
which.min(mean.cv.errors)
```

```{r}
reg.best <- regsubsets(status~., data = pd_data_1, nvmax = 22, method = "forward")
coef(reg.best, 17)
```

#### Backward Stepwise Selection 

```{r}
regfit.bwd.best <- regsubsets(status~., data = pd_train, nvmax = 22, method = "backward")
```

```{r}
val.errors <- rep(NA, 22)
for(i in 1:22){
  coefi <- coef(regfit.bwd.best, id = i)
  pred <- test.mat[, names(coefi)]%*%coefi
  val.errors[i] <- mean(as.numeric(pd_test$status)-pred^2)
}
which.min(val.errors)
```

```{r}
regfit.fwd.best <- regsubsets(status~., data = pd_data_1, nvmax = 22, method = "backward")
coef(regfit.fwd.best, 9)
```

We now try to choose among the models of different sizes using cross-validation.
```{r}
for(j in 1:k){
  best.fit <- regsubsets(status~., data = pd_data_1[folds!=j,], nvmax = 22, method = "backward")
  for(i in 1:22){
    pred <- predict.regsubsets(best.fit, pd_data_1[folds==j,], id=i)
    cv.errors[j,i] <- mean((as.numeric(pd_data_1$status)[folds==j]-pred)^2)
  }
}
```
```{r}
mean.cv.errors <- apply(cv.errors, 2, mean)
plot(mean.cv.errors, type = "b")
```
We can choose the 15-variable model. 

```{r}
reg.best <- regsubsets(status~., data = pd_data_1, nvmax = 22, method = "backward")
coef(reg.best, 15)
```

### Shrinkage {.tabset .tabset-fade .tabset-pills}

This approach involves fitting a model involving all p predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. shrinkage methods can also perform variable selection.

```{r}
library(glmnet)
x.train <- model.matrix(status~., data = pd_train)
x.test <- model.matrix(status~., data = pd_test)
y.train <- as.numeric(pd_train$status)
y.test <- as.numeric(pd_test$status)
```

#### Ridge Regression 

Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.

```{r}
set.seed(3573)
cv.out <- cv.glmnet(x.train, y.train, alpha = 0)
bestlam <- cv.out$lambda.min
bestlam
```
We see that the value of lambda that results in the smallest cross-validation error is 0.02309442

```{r}
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x.train, y.train, alpha = 0, lambda = grid)
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred-y.test)^2)
```
The test MSE associated with the best lambda value is 0.1308499

Finally, we refit our ridge regression model on the full data set, using the value of lambda chosen by cross-validation, and examine the coefficient estimates. 

```{r}
x <- model.matrix(status~., data = pd_data_1)
y <- as.numeric(pd_data_1$status)
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)
```
As expected, none of the coefficients are zero—ridge because regression does not
perform variable selection, thus we will now perform the LASSO. 

#### LASSO 

As with ridge regression, the lasso shrinks the coefficient estimates
towards zero. However, in the case of the lasso, the l1 penalty has the effect
of forcing some of the coefficient estimates to be exactly equal to zero when
the tuning parameter λ is sufficiently large. Hence, much like best subset selection,
the lasso performs variable selection.

```{r}
lasso.mod <- glmnet(x.train, y.train, alpha = 1, lambda = grid)
plot(lasso.mod)
```
We can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero.

We now perform cross-validation and compute the associated test error. 
```{r}
set.seed(3573)
cv.out <- cv.glmnet(x.train, y.train, alpha =1)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x.test)
mean((lasso.pred-y.test)^2)
```
We get a very similar test MSE to that of the ridge regression but slightly lower : 0.1278772 with the lambda chosen by cross-validation. 

```{r}
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)
lasso.coef
```
We see that 11 of 22 coefficient estimates are exactly zero. So the lasso model with lambda chosen by cross-validation contains only 11 variables. 

### Dimension Reduction {.tabset .tabset-fade .tabset-pills}

This approach involves projecting the p predictors into a M-dimensional subspace, where M <p. This is achieved by computing M different linear combinations, or projections, of the variables. Then these M projections are used as predictors to fit a linear regression model by least squares.

#### Principal Component Regression 

We perform PCR on the training data and evaluate its test test performance.
```{r}
library(pls)
set.seed(3573)
pcr.fit <- pcr(as.numeric(status)~., data = pd_train, scale = TRUE, validation = "CV")
```

The resulting fit can be examined using summary() or by plotting the CV scores
```{r}
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted
validationplot(pcr.fit, val.type = "MSEP")
```
```{r}
pcr.pred <- predict(pcr.fit, pd_test, ncomp = 19)
mean((pcr.pred-y.test)^2)
```
The test set MSE obtained (0.1731041) is higher than the one obtained using ridge regression and the lasso. Also, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or directly produce coefficients estimates.

#### Partial Least Squares 

```{r}
set.seed(3573)
pls.fit <- plsr(as.numeric(status)~., data = pd_train, scale = TRUE, validation = "CV")
validationplot(pls.fit, val.type = "MSEP")
```
```{r}
pls.pred <- predict(pls.fit, pd_test, ncomp = 9)
mean((pls.pred-y.test)^2)
```
The test MSE is comparable to, but slightly higher than, the test MSE obtained using PCR. 

## Feature Selection for linear models {.tabset .tabset-fade .tabset-pills}

### 11-variable model by LASSO {.tabset .tabset-fade .tabset-pills}

```{r}
lasso_data <- pd_data_1[, c(1,2,3,7,11,15,17,18,19,20,21,22)]
lasso_train <- pd_train[, c(1,2,3,7,11,15,17,18,19,20,21,22)]
lasso_test <- pd_test[, c(1,2,3,7,11,15,17,18,19,20,21,22)]
```

#### Logistic Regression 

```{r}
glm.fits <- glm(status~. , data = lasso_train, family = binomial )
```

```{r}
glm.probs <- predict(glm.fits, lasso_test, type = "response")
glm.pred <- rep(0, dim(lasso_test)[1])
glm.pred[glm.probs>.5] <- 1
table(glm.pred, lasso_test$status)
```
```{r}
logistic_error_lasso <- mean(glm.pred != lasso_test$status)
logistic_error_lasso
```
The test set error rate of the logistic model using the LASSO predictors is 23.5% 

#### Linear Discriminant Analysis

```{r}
lda.fit <- lda(status~., data = lasso_train[, -5])
```

```{r}
lda.pred <- predict(lda.fit, lasso_test)
lda.class <- lda.pred$class
table(lda.class, lasso_test$status)
```
```{r}
lda_error_lasso <- mean(lda.class != lasso_test$status)
lda_error_lasso
```
The test set error rate of the LDA model using LASSO predictors is is 17.6%. This shows a better performance than the logistic regression model.

#### Quadratic Discriminant Analysis 

```{r}
qda.fit <- qda(status~. , data = lasso_train)
```

```{r}
qda.class <- predict(qda.fit, lasso_test)$class
table(qda.class, lasso_test$status)
```

```{r}
qda_error_lasso <- mean(qda.class != lasso_test$status)
qda_error_lasso
```
The test set error rate of the QDA using the LASSO predictors is 8.8 % , thus this model performs better than both the logistic regression and LDA models.

#### KNN

```{r}
set.seed(3573)
knn.pred <- knn(lasso_train, lasso_test, lasso_train$status, k = 1)
table(knn.pred, lasso_test$status)
```

```{r}
knn_1_error_lasso <- 1-(12+46)/68
knn_1_error_lasso
```
The test set error rate using K=1 is 14.7 %

The best model for the 11-variable identified by LASSO is the QDA model with the lowest test set error. 

### 13-variable model by best subset seelction {.tabset .tabset-fade .tabset-pills}

```{r}
subset_data <- pd_data_1[, c(1,3,4,8,9,11,12,15,16,17,18,20,21,23)]
subset_train <- pd_train[, c(1,3,4,8,9,11,12,15,16,17,18,20,21,23)]
subset_test <- pd_test[, c(1,3,4,8,9,11,12,15,16,17,18,20,21,23)]
```

#### Logistic Regression 

```{r}
glm.fits <- glm(status~. , data = subset_train, family = binomial )
```
```{r}
glm.probs <- predict(glm.fits, subset_test, type = "response")
glm.pred <- rep(0, dim(subset_test)[1])
glm.pred[glm.probs>.5] <- 1
table(glm.pred, subset_test$status)
```
```{r}
logistic_error_subset <- mean(glm.pred != subset_test$status)
logistic_error_subset
```
The test set error rate of the logistic model using the subset predictors is 17.6% 

#### Linear Discriminant Analysis

```{r}
lda.fit <- lda(status~., data = subset_train[, -5])
```
```{r}
lda.pred <- predict(lda.fit, subset_test)
lda.class <- lda.pred$class
table(lda.class, subset_test$status)
```
```{r}
lda_error_subset <- mean(lda.class != subset_test$status)
lda_error_subset
```
The test set error rate of the LDA model using best subset predictors is 17.6%. This shows the same performance as the logistic regression model.

#### Quadratic Discriminant Analysis 

```{r}
qda.fit <- qda(status~. , data = subset_train)
```

```{r}
qda.class <- predict(qda.fit, subset_test)$class
table(qda.class, subset_test$status)
```

```{r}
qda_error_subset <- mean(qda.class != subset_test$status)
qda_error_subset
```
The test set error rate of the QDA using the subset predictors is 11.7 % , thus this model performs better than both the logistic regression and LDA models.

#### KNN

```{r}
set.seed(3573)
knn.pred <- knn(subset_train, subset_test, subset_train$status, k = 1)
table(knn.pred, subset_test$status)
```

```{r}
knn_1_error_subset <- 1-(13+49)/68
knn_1_error_subset
```
The test set error of the KNN using the subset predictors is 8.8 % 

The best model fit for the subset predictors is the KNN model with k=1.

### 17-variable model by forward selection {.tabset .tabset-fade .tabset-pills}

```{r}
forward_data <- pd_data_1[, c(1,3,4,5,7,8,9,12,13,15,16,17,18,19,20,21,22,23)]
forward_train <- pd_train[, c(1,3,4,5,7,8,9,12,13,15,16,17,18,19,20,21,22,23)]
forward_test <- pd_test[, c(1,3,4,5,7,8,9,12,13,15,16,17,18,19,20,21,22,23)]
```

#### Logistic Regression 

```{r}
glm.fits <- glm(status~. , data = forward_train, family = binomial )
```
```{r}
glm.probs <- predict(glm.fits, forward_test, type = "response")
glm.pred <- rep(0, dim(forward_test)[1])
glm.pred[glm.probs>.5] <- 1
table(glm.pred, forward_test$status)
```
```{r}
logistic_error_forward <- mean(glm.pred != forward_test$status)
logistic_error_forward
```
The test set error rate of the logistic model using the forward selection predictors is 17.6% 

#### Linear Discriminant Analysis

```{r}
lda.fit <- lda(status~., data = forward_train[, -4])
```
```{r}
lda.pred <- predict(lda.fit, forward_test)
lda.class <- lda.pred$class
table(lda.class, forward_test$status)
```
```{r}
lda_error_forward <- mean(lda.class != forward_test$status)
lda_error_forward
```
The test set error rate of the LDA model using forward selection predictors is 19.1%. This shows that the logistic regression model is better than the LDA model.

#### Quadratic Discriminant Analysis 

```{r}
qda.fit <- qda(status~. , data = forward_train)
```

```{r}
qda.class <- predict(qda.fit, forward_test)$class
table(qda.class, forward_test$status)
```

```{r}
qda_error_forward <- mean(qda.class != forward_test$status)
qda_error_forward
```
The test set error rate of the QDA using the forward selection predictors is 14.7 % , thus this model performs better than both the logistic regression and LDA models.

#### KNN

```{r}
set.seed(3573)
knn.pred <- knn(forward_train, forward_test, forward_train$status, k = 1)
table(knn.pred, forward_test$status)
```

```{r}
knn_1_error_forward <- 1-(13+49)/68
knn_1_error_forward
```
The test set error of the KNN using the subset predictors is 8.8 % 

The best model fit for the forward selection predictors is the KNN model. 

### 15-variable model by backward selection {.tabset .tabset-fade .tabset-pills}

```{r}
backward_data <- pd_data_1[, c(1,3,4,5,7,8,9,11,12,15,16,17,18,20,21,23)]
backward_train <- pd_train[, c(1,3,4,5,7,8,9,11,12,15,16,17,18,20,21,23)]
backward_test <- pd_test[, c(1,3,4,5,7,8,9,11,12,15,16,17,18,20,21,23)]
```

#### Logistic Regression 

```{r}
glm.fits <- glm(status~. , data = backward_train, family = binomial )
```
```{r}
glm.probs <- predict(glm.fits, backward_test, type = "response")
glm.pred <- rep(0, dim(backward_test)[1])
glm.pred[glm.probs>.5] <- 1
table(glm.pred, backward_test$status)
```
```{r}
logistic_error_backward <- mean(glm.pred != backward_test$status)
logistic_error_backward
```
The test set error rate of the logistic model using the backward selection predictors is 17.6% 

#### Linear Discriminant Analysis

```{r}
lda.fit <- lda(status~., data = backward_train[, -4])
```
```{r}
lda.pred <- predict(lda.fit, backward_test)
lda.class <- lda.pred$class
table(lda.class, backward_test$status)
```
```{r}
lda_error_backward <- mean(lda.class != backward_test$status)
lda_error_backward
```
The test set error rate of the LDA model using backward selection predictors is 17.6%. This shows the same performance as the logistic regression model.

#### Quadratic Discriminant Analysis 

```{r}
qda.fit <- qda(status~. , data = backward_train)
```

```{r}
qda.class <- predict(qda.fit, backward_test)$class
table(qda.class, backward_test$status)
```

```{r}
qda_error_backward <- mean(qda.class != backward_test$status)
qda_error_backward
```
The test set error rate of the QDA using the backward selection predictors is 11.7 % , thus this model performs better than both the logistic regression and LDA models.

#### KNN

```{r}
set.seed(3573)
knn.pred <- knn(backward_train, backward_test, backward_train$status, k = 1)
table(knn.pred, backward_test$status)
```

```{r}
knn_1_error_backward <- 1-(13+49)/68
knn_1_error_backward
```
The test set error of the KNN using the subset predictors is 8.8 % 

The best model fit for the backward selection predictors is the KNN model. 

## Non-Linear Models {.tabset .tabset-fade .tabset-pills}

### Classification Tree 

```{r}
library(tree)
set.seed(3573)
tree.pd <- tree(status~., pd_train)
tree.pred <- predict(tree.pd, pd_test, type = "class")
table(tree.pred, pd_test$status)
```
```{r}
error_tree <- 1 - (11+45)/68
error_tree
```
The test error of this classification tree is 17.6 %

```{r}
plot(tree.pd)
text(tree.pd, pretty = 0)
```

Next, we consider whether pruning the tree might lead to improved results.

```{r}
set.seed(3573)
cv.pd <- cv.tree(tree.pd, FUN = prune.misclass)
cv.pd
```
The tree with 7 terminal nodes results in the lowest cross-validation error rate, which is the initial tree so we do not need to prune. 

### Bagging 

```{r}
library(randomForest)
set.seed(3573)
bag.pd <- randomForest(status~., pd_train, mtry = 22, importance = TRUE)
bag.pd
```
We now need to assess how well does this bagged model perform on the test set.

```{r}
yhat.bag <- predict(bag.pd, pd_test)
plot(yhat.bag, pd_test$status)
abline(0, 1)
mean((as.numeric(yhat.bag)-y.test)^2)
```
The test MSE associated with the bagged classification tree is 0.16.

### Random Forest 

By default, randomForest() uses √p variables when building a random forest of classification trees. Here we use mtry = √22 = approx. 5
```{r}
set.seed(3573)
rf.pd <- randomForest(status~., pd_train, mtry = 5, importance = TRUE)
yhat.rf <- predict(rf.pd, pd_test)
mean((as.numeric(yhat.rf)-y.test)^2)
```
The test MSE is 0.13; this indicates that random forests yielded an improvement over bagging in this case.

We can plot the importance measures of each variable.
```{r}
varImpPlot(rf.pd)
```
### Boosting 

```{r}
library(gbm)
set.seed(3573)
boost.pd <- gbm(status~., pd_train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
```

```{r}
summary(boost.pd)
```
We can see that PPE, D2 and spread2 are the three most important variables.

We now use the boosted model to predict status on the test set
```{r}
yhat.boost <- predict(boost.pd, pd_test, n.trees = 5000)
mean((yhat.boost-y.test)^2)
```
The test MSE obtained is 0.077; this indicates that boosting yielded an improvement over bagging and random forests.

```{r}
# library(adabag)
# boosting_error <- boosting.cv(status~., pd_train, v = 10, mfinal = 100)$error
# boosting_error
```

# Second Analysis 

```{r}
pd_data_combined <- read.csv("~/GitLab/BIF524_Fall20_ElFeghali_Melissa/Final_Project/Parkinson_data.csv", header = TRUE, sep = ",", dec = ".")
rownames(pd_data_combined) <- pd_data_combined[,1] 
pd_data_combined <- pd_data_combined[,-1]

names=substr(rownames(pd_data_combined), 0,12)
j <- 1
matrix <- c()
rnames <- c()
for (i in 1:length(names))
{
  if (is.na(names[i+1]) || names[i+1]!=names[i])
  {
    matrix <- rbind(matrix, sapply(pd_data_combined[j:i,], mean))
    rnames <- c(rnames, names[i])
    j <- i+1
  }
}
rownames(matrix) <- rnames
pd_data_combined <- as.data.frame(matrix)
```

```{r}
pd_data_combined$status <- as.factor(pd_data_combined$status)
  
set.seed(3573)

sample_2 <- sample.split(pd_data_combined, SplitRatio = 2/3)
pd_combined_train <- subset(pd_data_combined, sample == TRUE)
pd_combined_test <- subset(pd_data_combined, sample == FALSE) 
```

## Linear Models {.tabset .tabset-fade .tabset-pills}

### Logistic Regression 

We now fit a logistic regression model using only the subset of the observations. 
```{r}
glm.fits.combined <- glm(status~. , data = pd_combined_train, family = binomial )
```

```{r}
glm.probs.combined <- predict(glm.fits.combined, pd_combined_test, type = "response")
glm.pred.combined <- rep(0, dim(pd_combined_test)[1])
glm.pred.combined[glm.probs.combined>.5] <- 1
table(glm.pred.combined, pd_combined_test$status)
```
```{r}
logistic_combined_error <- mean(glm.pred.combined != pd_combined_test$status)
logistic_combined_error
```
The test set error rate of the logistic model is 40% 

```{r}
library(boot)
cv.err.combined <- cv.glm(pd_combined_train, glm.fits.combined)
logistic_CV_combined_error <- cv.err$delta
logistic_CV_combined_error
```
[1] 0.1461381 0.1460604

### Linear Discriminant Analysis 

Now, we will perform LDA on the training data set.

```{r}
library(MASS)
lda.fit.combined <- lda(status~., data = pd_combined_train[, -5])
lda.fit.combined
```

```{r}
lda.pred.combined <- predict(lda.fit.combined, pd_combined_test)
lda.class.combined <- lda.pred.combined$class
table(lda.class.combined, pd_combined_test$status)
```
```{r}
lda_error_combined <- mean(lda.class.combined != pd_combined_test$status)
lda_error_combined
```
The test set error rate of the LDA model is 50% 

### K-Nearest Neighbors 

We will now perform KNN. 
```{r}
library(class)

set.seed(3573)
knn.pred.combined <- knn(pd_combined_train, pd_combined_test, pd_combined_train$status, k = 1)
table(knn.pred.combined, pd_combined_test$status)
```
```{r}
knn_1_combined_error <- 1-(2+4)/10
knn_1_combined_error
```
The test set error rate using K=1 is 40 %

We now repeat the analysis using K = 3
```{r}
set.seed(3573)
knn.pred.combined <- knn(pd_combined_train, pd_combined_test, pd_combined_train$status, k = 3)
table(knn.pred.combined, pd_combined_test$status)
```
```{r}
knn_3_error <- 1-(1+5)/10
knn_3_error
```
Increasing K does not provides us with better results. 


## Linear Model Selection and Regularization 

### Shrinkage {.tabset .tabset-fade .tabset-pills}

```{r}
library(glmnet)
x.train.combined <- model.matrix(status~., data = pd_combined_train)
x.test.combined <- model.matrix(status~., data = pd_combined_test)
y.train.combined <- as.numeric(pd_combined_train$status)
y.test.combined <- as.numeric(pd_combined_test$status)
```

#### Ridge Regression 

Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.

```{r}
set.seed(3573)
cv.out.combined <- cv.glmnet(x.train.combined, y.train.combined, alpha = 0)
bestlam.combined <- cv.out.combined$lambda.min
bestlam.combined
```
We see that the value of lambda that results in the smallest cross-validation error is 1.801219

```{r}
grid <- 10^seq(10, -2, length = 100)
ridge.mod.combined <- glmnet(x.train.combined, y.train.combined, alpha = 0, lambda = grid)
ridge.pred.combined <- predict(ridge.mod.combined, s = bestlam.combined, newx = x.test.combined)
mean((ridge.pred.combined-y.test.combined)^2)
```
The test MSE associated with the best lambda value is 0.2959128

Finally, we refit our ridge regression model on the full data set, using the value of lambda chosen by cross-validation, and examine the coefficient estimates. 

```{r}
x.combined <- model.matrix(status~., data = pd_data_combined)
y.combined <- as.numeric(pd_data_combined$status)
out.combined <- glmnet(x.combined, y.combined, alpha = 0)
predict(out.combined, type = "coefficients", s = bestlam.combined)
```
As expected, none of the coefficients are zero—ridge because regression does not
perform variable selection, thus we will now perform the LASSO. 

#### LASSO 

```{r}
lasso.mod.combined <- glmnet(x.train.combined, y.train.combined, alpha = 1, lambda = grid)
plot(lasso.mod.combined)
```
We can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero.

We now perform cross-validation and compute the associated test error. 
```{r}
set.seed(3573)
cv.out.combined <- cv.glmnet(x.train.combined, y.train.combined, alpha =1)
bestlam.combined <- cv.out.combined$lambda.min
lasso.pred.combined <- predict(lasso.mod.combined, s = bestlam.combined, newx = x.test.combined)
mean((lasso.pred.combined-y.test.combined)^2)
```
We get a very similar test MSE to that of the ridge regression but slightly lower : 0.2752583 with the lambda chosen by cross-validation. 

```{r}
out.combined <- glmnet(x.combined , y.combined , alpha = 1, lambda = grid)
lasso.coef.combined  <- predict(out.combined , type = "coefficients", s = bestlam.combined )
lasso.coef.combined 
```
We see that 15 of 22 coefficient estimates are exactly zero. So the lasso model with lambda chosen by cross-validation contains only 7 variables. 

### Dimension Reduction {.tabset .tabset-fade .tabset-pills}

#### Principal Component Regression 

We perform PCR on the training data and evaluate its test test performance.
```{r}
library(pls)
set.seed(3573)
pcr.fit.combined  <- pcr(as.numeric(status)~., data = pd_combined_train, scale = TRUE, validation = "CV")
```

The resulting fit can be examined using summary() or by plotting the CV scores
```{r}
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted
validationplot(pcr.fit.combined , val.type = "MSEP")
```
```{r}
pcr.pred.combined  <- predict(pcr.fit.combined , pd_combined_test, ncomp = 1)
mean((pcr.pred.combined -y.test.combined )^2)
```
The test set MSE obtained (0.313282) is higher than the one obtained using ridge regression and the lasso. Also, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or directly produce coefficients estimates.

#### Partial Least Squares 

```{r}
set.seed(3573)
pls.fit.combined <- plsr(as.numeric(status)~., data = pd_combined_train, scale = TRUE, validation = "CV")
validationplot(pls.fit.combined, val.type = "MSEP")
```
```{r}
pls.pred.combined <- predict(pls.fit.combined, pd_combined_test, ncomp = 1)
mean((pls.pred.combined-y.test.combined)^2)
```
The test MSE (0.2824686) is lower than that of the test MSE obtained using PCR. 

## Feature Selection for linear models {.tabset .tabset-fade .tabset-pills}

### 7-variable model by LASSO {.tabset .tabset-fade .tabset-pills}

```{r}
lasso_data_combined <- pd_data_combined[, c(2,3,15,17,19,20,21,22)]
lasso_train_combined <- pd_combined_train[, c(2,3,15,17,19,20,21,22)]
lasso_test_combined <- pd_combined_test[, c(2,3,15,17,19,20,21,22)]
```

#### Logistic Regression 

```{r}
glm.fits.combined <- glm(status~. , data = lasso_train_combined, family = binomial )
```

```{r}
glm.probs.combined <- predict(glm.fits.combined, lasso_test_combined, type = "response")
glm.pred.combined <- rep(0, dim(lasso_test_combined)[1])
glm.pred.combined[glm.probs.combined>.5] <- 1
table(glm.pred.combined, lasso_test_combined$status)
```
```{r}
logistic_error_lasso_combined <- mean(glm.pred.combined != lasso_test_combined$status)
logistic_error_lasso_combined
```
The test set error rate of the logistic model using the LASSO predictors is 50% 

#### Linear Discriminant Analysis

```{r}
lda.fit.combined <- lda(status~., data = lasso_train_combined)
```

```{r}
lda.pred.combined <- predict(lda.fit.combined, lasso_test_combined)
lda.class.combined <- lda.pred.combined$class
table(lda.class.combined, lasso_test_combined$status)
```
```{r}
lda_error_lasso_combined <- mean(lda.class.combined != lasso_test_combined$status)
lda_error_lasso_combined
```
The test set error rate of the LDA model using LASSO predictors is 40%. This shows a better performance than the logistic regression model.

#### KNN

```{r}
set.seed(3573)
knn.pred.combined <- knn(lasso_train_combined, lasso_test_combined, lasso_train_combined$status, k = 1)
table(knn.pred.combined, lasso_test_combined$status)
```

```{r}
knn_1_error_lasso_combined <- 1-(2+4)/10
knn_1_error_lasso_combined
```
The test set error rate using K=1 is 40 %

The best model for the 7-variable identified by LASSO is the QDA model.

## Non-Linear Models {.tabset .tabset-fade .tabset-pills}

### Bagging 

```{r}
library(randomForest)
set.seed(3573)
bag.pd.combined <- randomForest(status~., pd_combined_train, mtry = 22, importance = TRUE)
bag.pd.combined
```
We now need to assess how well does this bagged model perform on the test set.

```{r}
yhat.bag.combined <- predict(bag.pd.combined, pd_combined_test)
plot(yhat.bag.combined, pd_combined_test$status)
abline(0, 1)
mean((as.numeric(yhat.bag.combined)-y.test.combined)^2)
```
The test MSE associated with the bagged classification tree is 0.3

### Random Forest 

By default, randomForest() uses √p variables when building a random forest of classification trees. Here we use mtry = √22 = approx. 5
```{r}
set.seed(3573)
rf.pd.combined <- randomForest(status~., pd_combined_train, mtry = 5, importance = TRUE)
yhat.rf.combined <- predict(rf.pd.combined, pd_combined_test)
mean((as.numeric(yhat.rf.combined)-y.test.combined)^2)
```
The test MSE is 0.3; this indicates that random forests did not yield any improvement over bagging in this case.

We can plot the importance measures of each variable.
```{r}
varImpPlot(rf.pd.combined)
```